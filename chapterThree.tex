\chapter[Optimizing Optane PMem Performance for Serverless Storage]{Optimizing Optane PMem Performance for Serverless Storage}

As discussed earlier, the introduction of Optane PMem presents a significant opportunity for serverless storage services. This memory technology offers a unique combination of cost-effective high capacity, high performance, and support for data persistence \cite{IntelOp15:online}. When utilized in App-Direct mode, Optane PMem DIMMs and DRAM DIMMs function as independent memory resources directly controlled by applications for load/store operations. This configuration allows Optane PMem capacity to serve as byte-addressable persistent memory mapped into the system application space, providing direct accessibility to applications. Consequently, Optane PMem can function as persistent storage with memory-like speeds.

However, resource contention within Optane PMem can lead to substantial performance and contractual implications for multi-tenant serverless storage services. The limited capability of Optane PMem to handle accesses from multiple threads can result in degraded system performance during workload spikes, undermining the hallmark autoscaling features of serverless computing. Moreover, in multi-user environments, where physical resources are shared among tenants, performance variations due to Optane PMem contention can hinder service providers from offering consistent service level agreements.

To mitigate contention effects, prior research recommends restricting the number of threads accessing Optane PMem simultaneously. For instance, Yang et al. \cite{yang2020empirical} enhanced the performance of an PM-aware file system by limiting the number of writer threads accessing each Optane PMem DIMM. Similarly, Ribbon \cite{wu2020ribbon} dynamically adjusts the number of threads performing cache line flushing (CLF) operations at runtime. While effective, such approaches introduce management complexities for system administrators managing multi-tenant serverless storage environments.

Given the intricate nature of FaaS workloads, implementing efficient concurrency control mechanisms to optimize an Optane PMem-based serverless storage service is challenging. These challenges are elaborated in Section 3.1, but, in summary, service providers face three key tasks: ensuring predictable performance to meet diverse SLAs, transparently scaling resources to match current workload demands, and swiftly adapting to sudden workload shifts. To address these challenges, we propose a solution that shoulders these responsibilities on behalf of service providers.

This chapter introduces the NVM Middleware, a middleware optimization layer positioned between a serverless storage service and Intel Optane PMem. Its primary objective is to address the limitations associated with Optane PMem while simultaneously meeting the diverse (SLAs) required by multiple tenants amid varying workloads. Additionally, the chapter describes the development of a reinforcement learning agent to enable the NVM Middleware to quickly adapt to changing workloads. This agent considers workload characteristics and SLAs metrics, learning from past experiences to dynamically scale resources.
% The NVM Middleware distinguishes between latency-critical and throughput-oriented workloads, employing tailored concurrency control mechanisms for each category. This approach minimizes contention on the memory device and reduces interference among workloads with differing SLAs. Additionally, we propose the development of a reinforcement learning agent to enable the NVM Middleware to quickly adapt to changing workloads. This agent considers workload characteristics and SLAs, learning from past experiences to dynamically scale resources.

The subsequent sections of this chapter elaborate on the design objectives of the NVM Middleware, providing insights into its architectural principles and programming interface. Furthermore, the chapter delves into the implementation aspects of the reinforcement learning model, elucidating the training methodology employed to equip an agent with the capability to dynamically adjust the NVM Middleware's concurrency levels in response to fluctuating workloads. Lastly, the chapter provides an in-depth discussion on the implementation intricacies of the NVM Middleware.

\section{Motivation}

In this section, we discuss the pain points of controlling concurrency levels to optimize Optane PMem within a serverless storage service and explain the design goals of the NVM Middleware.

\subsection{Concurrency Control Challenges in Serverless Storage}

When building an Optane PMem-based serverless storage service, optimizing the memory's performance is just the start. Early works in serverless computing have identified several tasks that a storage service must perform efficiently to meet the demands of serverless computing \cite{180275,jonas2019cloud,klimovic2018understanding,klimovic2018pocket,wu2019autoscaling,romero2021faat}. As a result, service providers must ensure that their concurrency control policies do not interfere with these design goals. In this work, we focus on three challenges faced by service providers when designing a high-performance storage service.

\subsubsection{Support for a wide heterogeneity of applications}

In serverless computing, applications are often deployed as a set of serverless functions that interact with remote storage for data sharing. Previous research indicates significant diversity among these applications concerning data access size \cite{klimovic2018pocket,romero2021faat}, access patterns \cite{romero2021faat}, and performance requirements \cite{180275,jonas2019cloud}. Consequently, service providers encounter the challenge of adjusting concurrency levels to accommodate various application types. In this study, we focus on two distinct application categories: interactive applications, which prioritize low-access latency, and batch applications, which emphasize achieving high throughput.

% In this work, we argue that considering the workload characteristics is key for tuning the system efficiently. The allocation of resources can vary depending on the workload type.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth,height=\textheight,keepaspectratio,angle=0]{images/measurement-study-latency.png}
  \includegraphics[width=0.7\textwidth,height=\textheight,keepaspectratio,angle=0]{images/measurement-study-tp.png}
  \caption[Impact of Concurrent Applications on Optane PMem 99th Latency and Throughput]{Comparison of 99th latency and throughput variations observed in Optane DC PMem when running YCSB and SVD applications concurrently. The top image depicts the latency comparison between YCSB running alone and concurrently with SVD, while the bottom image illustrates the throughput comparison between SVD running alone and concurrently with YCSB. Notably, concurrent application execution significantly impacts the 99th latency compared to throughput.}
  \label{fig:measurement_study}
\end{figure}

A measurement study was conducted to evaluate the effects of concurrent interactive and batch applications sharing Optane PMem. The study utilized the applications and experimental platform detailed in Section 4. The results are illustrated in Figure \ref{fig:measurement_study}. In this experiment, both an interactive application (YCSB) and a batch application (SVD) were executed individually and concurrently, utilizing Optane PMem as their storage medium. The findings illustrate a noticeable impact on both throughput and 99th latency when running the applications concurrently. Particularly, latency is significantly affected compared to throughput, suggesting contention among tenant workloads sharing Optane PMem. Hence, effective concurrency control mechanisms are essential to mitigate such contention, while reaching a balance between latency and throughput exhibited.

\subsubsection{Compliance with Service Level Agreements}

The success of a storage service relies on its ability to comply with various SLAs. SLAs play a critical role in governing the relationship between the storage provider and its customers. They help establish clear expectations between both parties regarding the quality of storage service. Therefore, service providers face the challenge of staying in compliance with these SLAs while they seek to optimize Optane PMem. 

\subsubsection{Automatic and transparent scaling}

Serverless workloads are extremely unpredictable. These workloads can launch hundreds of functions instantaneously to meet application demands \cite{klimovic2018understanding}. Furthermore, the data access patterns of the applications can change dramatically over time \cite{romero2021faat,wu2019autoscaling}. Service providers face the challenge of scaling the resources, such as number of threads, automatically to meet the demands of changing workloads. In addition, they must ensure that the system adapts quickly enough to avoid missing SLAs.

\subsection{NVM Middleware Design Overview}

We design NVM Middleware with two main design goals.

\textbf{Workload-aware Contention Management.} The NVM Middleware leverage insights about the workload characteristics and performance requirements of applications to make informed decisions about resource allocation and contention resolution. It dynamically adjusts resource allocation for each application type independently, mitigating performance degradation caused by contention and enabling efficient resource sharing among co-located applications.. This adaptive approach enables the NVM Middleware to allocate resources judiciously to maximize overall system efficiency and meet diverse performance requirements of both interactive and batch applications. By using the workload-aware contention management offered by the NVM Middleware, a storage system using Optane PMem can effectively balance the needs of different workload types, ensuring optimal performance and fair share of resources.

\textbf{RL-driven autoscaling policies.} Our solution proposes the use of Reinforcement Learning to dynamically scale resources, learning from historical data and anticipating changes in workload patterns. By incorporating information from SLAs to guide the learning process, the NVM Middleware can quickly adapt to changing worload patterns over time and meet SLA objectives more effectively than traditional threshold-based approaches \cite{cano2017curator}. Moreover, given the dynamic and unpredictable nature of FaaS workloads, we propose a model-free algorithm, Q-Learning, to continuously learn the optimal policy based on observed experiences, allowing the NVM Middleware to adapt to new scenarios without needing to explicitly model them.

\section{NVM Middleware Architecture}

\begin{figure}[ht]
  \centering
  \includegraphics[scale=1]{images/nvm_design.png}
  \caption[NVM Middleware Architecture]{Illustration depicting the architecture of the NVM Middleware, which serves as an optimization layer for Optane PMem. The NVM Middleware governs the concurrency level on the device, categorizing incoming I/O requests based on their type (blue for interactive and orange for batch) and assigning distinct worker threads for interactive or batch workloads. The number of threads for each workload type is dynamically configurable to adapt to the system's requirements.}
  \label{fig:nvm_architecture}
\end{figure}

Figure \ref{fig:nvm_architecture} provides an overview of the NVM Middleware architecture. Positioned as a middle layer connecting user applications with Optane PMem, its design is tailored for seamless integration within a storage service, serving as an optimization layer specifically targeting Optane PMem. It comprises a request handler, two concurrency thread pools, and a monitoring and resource management module.

The request handler serves as the primary interface for handling user I/O requests. Upon receipt, it segregates requests into two distinct non-blocking First-In-First-Out (FIFO) queues: one tailed for interactive requests and the other for batch ones. Leveraging insights into workload characteristics, the handler intelligently allocates requests to the appropriate queue. Moreover, each queue is assigned a dedicated pool of worker threads tasked with dispatching I/O requests to Optane PMem using PMEMKV. These worker threads are categorized as either interactive or batch threads. Notably, these thread pools operate independently and are dynamically managed and scaled by the Reinforcement Learning agent to meet predetermined latency and throughput goals.

The Monitoring and Resource Management module offers an interface to monitor system load and SLA performance metrics. This module initiates a separate control thread tasked with gathering data on key parameters within the NVM Middleware, such as 99th latency, throughput, and system load. Utilizing this information, the RL agent makes data-driven decisions regarding optimal thread pool scaling. Subsequently, these decisions are communicated to the Monitoring and Resource Management module, which executes the required actions within the NVM Middleware.

\section{NVM Middleware Programming Interface}

\begin{table}[ht]
  \centering
  % \caption[NVM Middleware Programming Interface]{Overview of the NVM Middleware programming interface, categorized by functionality. $System$ functions facilitate resource management within the middleware, including database initialization and thread pool management. $Storage$ functions provide a fundamental key-value interface for submitting requests aimed at accessing persistent memory. $RL$ functions provide utilities for the reinforcement learning process, enabling monitoring of system statistics, state retrieval, and triggering scaling actions.}
  \caption[NVM Middleware Programming Interface]{Overview of the NVM Middleware programming interface, categorized by functionality. $System$ functions facilitate resource management within the NVM Middleware, including database initialization and thread pool management. $Storage$ functions provide a fundamental key-value interface for submitting requests aimed at accessing persistent memory. $RL$ functions provide utilities for the reinforcement learning process, enabling monitoring of system statistics, state retrieval, and triggering scaling actions.}
  \label{table:programming_interface}
  % Tabular environment goes AFTER the caption!
  \begin{adjustbox}{width=1\textwidth}
  \begin{tabular}{|l|l|l|}
    % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
    \hline
    \thead{Category} & \thead{API Name} & \thead{Functionality} \\
    \hline
    System & start(db, interactiveThreads, batchThreads) & \makecell[cl]{Create the PMEMKV database, start thread pools, \\ and initiate NVM Middleware's monitoring.} \\
    \hline
    System & stop() & \makecell[cl]{Close PMEMKV database, stop thread pools, \\ and stop NVM Middleware's monitoring.} \\
    \hline
    Storage & get(key, mode) & \makecell[cl] {Submits request to retrieve a key from \\ persistent memory.} \\
    \hline
    Storage & put(key, value, mode) & \makecell[cl] {Submits request to write a key to \\ persistent memory.} \\
    \hline
    RL & get\_stats() & \makecell[cl] {Provides the 99th percentile and throughput \\ observed by the NVM Middleware.} \\
    \hline
    RL & get\_state() & \makecell[cl] {Provides the current state within the \\ NVM Middleware.} \\
    \hline
    RL & perform\_action(action) & Triggers a scaling action. \\
    \hline
  \end{tabular}
\end{adjustbox}
\end{table}


Table \ref{table:programming_interface} outlines the NVM Middleware's programming interface, presenting a set of functions designed to facilitate interaction with a storage system and the reinforcement learning agent. 

The $start$ function initializes the PMEMKV database, initializes the thread pools with an initial thread count, and triggers the system monitoring within the Monitoring and Resource Management Module. In contrast, the $stop$ function terminates the database connection, halts all threads in the thread pools, and stops system monitoring. Furthermore, the $get$ and $put$ functions facilitate key-value interactions with the persistent memory, allowing for read and write operations. These functions are designed to accommodate hints regarding the request type (e.g., latency-sensitive or throughput-oriented), aiding the request handler in queue allocation.

The $get\_stats$ function furnishes insights into the 99th percentile and throughput observed by the NVM Middleware at any given moment. Similarly, the $get\_state$ function provides real-time state information as outlined in Table \ref{table:state_space}. Finally, the $perform\_action$ function accepts scaling actions detailed in Table \ref{table:action_space} and initiates the corresponding action within the NVM Middleware.

\section{Reinforcement Learning Component}

In this section, we discuss the Q-learning algorithm used by the RL agent to dynamically adjust the number of threads assigned to each thread pool. The agentâ€™s goal is to find the best combination of threads that meets predetermined latency and throughput SLAs while minimizing contention and ensuring efficient utilization of Optane PMem. 

\subsection{Integration with the NVM Middleware}

\begin{figure}[ht]
  \centering
  \includegraphics[scale=1]{images/rl_workflow.png}
  \caption[Interaction between Reinforcement Learning Agent and NVM Middleware]{Interaction between the RL agent and the NVM Middleware. At each time step, the RL agent observes (I) the state of the environment and takes a scaling action (II) on the NVM Middleware based on its learned policy. Subsequently, the agent receives a reward (III), which is utilized to iteratively refine its policy.}
  \label{fig:rl_workflow}
\end{figure}

Figure \ref{fig:rl_workflow} offers a visual representation of the interaction between the RL agent and the NVM Middleware. At each time step, the NVM Middleware receives a diverse influx of requests, spanning both interactive and batch tasks. These requests necessitate translation into actionable I/O commands directed towards the Optane PMem.

Concurrently, the RL agent adeptly captures the environment's current state, leveraging real-time workloads' characteristics and performance metrics provided by the monitoring module. Utilizing this information, the agent orchestrates the selection of an optimal action, guiding the dynamic adjustment of threads within the interactive and batch thread pools. This adaptive decision-making process is exemplified by actions like augmenting the count of interactive threads to address evolving workload demands.

Following action selection, the NVM Middleware's resource management module implements the chosen course of action, fine-tuning the NVM Middleware's interactive and batch threads to efficiently handle incoming user requests. Upon the completion of each time step, the action's effectiveness is rigorously assessed against predefined SLA targets, yielding a reward signal generated by a reward manager.

The reward serves as invaluable feedback for the RL agent, empowering iterative policy updates aimed at refining decision-making strategies in subsequent time steps. Thus, the presented framework embodies a recursive learning cycle, wherein the RL agent continuously hones its behavior through real-world interactions, ensuring adaptive responsiveness to evolving workload dynamics.

\subsection{Reinforcement Learning Model}

\subsubsection{State Space}

\begin{table}[ht]
  \centering
  \caption[Reinforcement Learning State Representation]{State representation for the reinforcement learning model. The number of interactive and batch threads is capped at 32, as measurements showed performance degradation with more than 32 threads. The interactiveQueueSize and batchQueueSize represent the number of requests in the respective queues within the NVM Middleware. The block sizes and write ratios are determined by analyzing the I/O requests received from client workloads.}
  \label{table:state_space}
  \begin{adjustbox}{width=1\textwidth}
  \begin{tabular}{|l|l|l|}
    \hline
    \thead{Name} & \thead{Description} & \thead{Values} \\
    \hline
    interactiveThreads & \makecell[l]{Number of interactive threads assigned\\ to the interactive thread pool.} & $1 \leq \text{interactiveThreads} \leq 32$ \\
    \hline
    batchThreads & \makecell[l]{Number of batch threads assigned\\ to the batch thread pool.} & $1 \leq \text{batchThreads} \leq 32$ \\
    \hline
    interactiveQueueSize & \makecell[l]{Number of requests in the interactive queue.} & $\text{interactiveQueueSize} \in \mathbb{R}^+$ \\
    \hline
    batchQueueSize & \makecell[l]{Number of requests in the batch queue.} & $\text{batchQueueSize} \in \mathbb{R}^+$ \\
    \hline
    interactiveBlockSize & \makecell[l]{Average data access size of interactive workload.} & $\text{interactiveBlockSize} \in \mathbb{R}^+$ \\
    \hline
    batchBlockSize & \makecell[l]{Average data access size of batch workload.} & $\text{batchBlockSize} \in \mathbb{R}^+$ \\
    \hline
    interactiveWriteRatio & \makecell[l]{Proportion of write requests compared\\ to read requests in the interactive workload.} & $\text{interactiveWRatio} \in \mathbb{R}^+$ \\
    \hline
    batchWriteRatio & \makecell[l]{Proportion of write requests compared\\ to read requests in the batch workload.} & $\text{batchWRatio} \in \mathbb{R}^+$ \\
    \hline
  \end{tabular}
  \end{adjustbox}
\end{table}

Table \ref{table:state_space} presents the features of our state representation. At each time step $t$, we define the state $s_t$ as a tuple:

\[
\begin{aligned}
s_t = (& \text{interactiveThreads}_t, \text{batchThreads}_t, \text{InteractiveQueueSize}_t, \text{batchQueueSize}_t, \\
& \text{interactiveBlockSize}_t, \text{batchBlockSize}_t, \text{interactiveWRatio}_t, \text{batchWRatio}_t )
\end{aligned}
\]

where $s_t \in S$ represents the state space. The tuple encapsulates the key features characterizing the system's current state, including the number of interactive and batch threads, number of pending requests in the NVM Middleware's queues, individual workload data access sizes, and write ratio for both interactive and batch workloads.

\subsubsection{Action Space}

\begin{table}[ht]
  \centering
  \caption[Reinforcement Learning Action Space]{Possible actions in the action space of reinforcement learning and their effects on the NVM Middleware's concurrency control mechanism.}
  \label{table:action_space}
  \begin{tabular}{|c|l|l|}
  \hline
  \thead{Action} & \thead{Effect on \ Interactive Threads} & \thead{Effect on \ Batch Threads} \\
  \hline
  0 & No change & No change \\
  1 & Increase by 1 & No change \\
  2 & Decrease by 1 & No change \\
  3 & No change & Increase by 1 \\
  4 & No change & Decrease by 1 \\
  5 & Increase by 1 & Increase by 1 \\
  6 & Increase by 1 & Decrease by 1 \\
  7 & Decrease by 1 & Increase by 1 \\
  8 & Decrease by 1 & Decrease by 1 \\
  \hline
  \end{tabular}
\end{table}

  Table \ref{table:action_space} illustrates the feasible actions within the action space. Each action corresponds to a potential adjustment in the number of interactive and batch threads. The table enumerates nine distinct actions, each with its respective effect on the number of interactive threads and batch threads.

  Mathematically, the set of actions $A$ is defined as $A = \{0,1,2,3,4,5,6,7,8\}$ for a given state $s_t \in S$.

\subsubsection{Reward}

To guide the optimization process of the reinforcement learning agent, we establish an algorithm (Algorithm \ref{algo:reward_calculation}) to calculate a reward value based on observed and target latency and throughput metrics. This algorithm, outlined below, serves as a crucial component in training the RL agent to make informed decisions.

\begin{enumerate}
  \item Lines 1-5 define goals, scaling factors, and penalties. The observed and target latency ($lat$, $lat\_goal$) and throughput ($tp$, $tp\_goal$) metrics are scaled to a normalized range using scaling factors ($max\_scale\_lat$, $max\_scale\_tp$) and minimum scale ($min\_scale$). This normalization process ensures that both metrics contribute proportionally to the reward calculation.
  \item Lines 6-7 compare the scaled latency ($lat$) and throughput ($tp$) metrics against the scaled target values for latency ($lat\_goal$) and throughput ($tp\_goal$). The absolute differences between observed and target values are computed to quantify the error in latency ($error\_lat$) and throughput ($error\_tp$).
  \item Lines 8-12 determine the reward based on three distinct scenarios. Firstly, if both latency and throughput goals are achieved, a high positive reward is assigned. Secondly, if both goals are not met, a low negative reward is assigned, taking into account both latency and throughput errors. The disparity in penalties, represented by $lat\_penalty$ and $tp\_penalty$, ensures that both types of errors contribute proportionately to the overall reward. Thirdly, if only the latency goal remains unmet, a low negative reward is assigned, incorporating the latency penalty and error. Finally, if only the throughput goal is unmet, a similar low negative reward is assigned, encompassing the throughput penalty and error.
\end{enumerate}

\begin{algorithm}
  \small
  \caption{Reward Calculation Algorithm}
  \label{algo:reward_calculation}
  \SetAlgoLined
  \KwIn{System statistics: $\text{stat}$}
  \KwOut{Reward value: $\text{reward}$}
  \tcc{Initialize variables}
  $\text{max\_scale\_lat} \leftarrow 1000$, $\text{max\_scale\_tp} \leftarrow 10$, $\text{min\_scale} \leftarrow 1$, $\text{lat\_goal} \leftarrow 200$, $\text{tp\_goal} \leftarrow 250000$, $\text{lat\_penalty} \leftarrow 500.0$, $\text{tp\_penalty} \leftarrow 5000.0$\;
    
  \tcc{Scale observed and target latency and throughput}
  $\text{lat} \leftarrow ((\text{max\_scale\_lat} - \text{min\_scale}) \times (\text{stat.tailLatency} - \text{min\_value}) / (\text{max\_latency} - \text{min\_value})) + \text{min\_scale}$\;
  $\text{tp} \leftarrow ((\text{max\_scale\_tp} - \text{min\_scale}) \times (\text{stat.throughput} - \text{min\_value}) / (\text{max\_throughput} - \text{min\_value})) + \text{min\_scale}$\;
  $\text{lat\_goal} \leftarrow ((\text{max\_scale\_lat} - \text{min\_scale}) \times (\text{lat\_goal} - \text{min\_value}) / (\text{max\_latency} - \text{min\_value})) + \text{min\_scale}$\;
  $\text{tp\_goal} \leftarrow ((\text{max\_scale\_tp} - \text{min\_scale}) \times (\text{tp\_goal} - \text{min\_value}) / (\text{max\_throughput} - \text{min\_value})) + \text{min\_scale}$\;
    
  \tcc{Calculate errors}
  $\text{error\_lat} \leftarrow |\text{lat} - \text{lat\_goal}|$\;
  $\text{error\_tp} \leftarrow |\text{tp} - \text{tp\_goal}|$\;
    
  \tcc{Calculate reward}
  \eIf{$\text{lat} \leq \text{lat\_goal}$ \textbf{and} $\text{tp} \geq \text{tp\_goal}$}{
      $\text{reward} \leftarrow 10 \times (\text{error\_lat} + \text{error\_tp}$) \tcp*{High reward for meeting both latency and throughput goals}
  }{
      \eIf{$\text{lat} > \text{lat\_goal}$ \textbf{and} $\text{tp} < \text{tp\_goal}$}{
          $\text{reward} \leftarrow -1 \times (\text{lat\_penalty} \times \text{error\_lat} + \text{tp\_penalty} \times \text{error\_tp})$ \tcp*{Penalize for high latency and low throughput}
      }{
          \eIf{$\text{lat} > \text{lat\_goal}$}{
              $\text{reward} \leftarrow -1 \times \text{lat\_penalty} \times \text{error\_lat}$ \tcp*{Penalize for high latency}
          }{
              $\text{reward} \leftarrow -1 \times \text{tp\_penalty} \times \text{error\_tp}$ \tcp*{Penalize for low throughput}
          }
      }
  }
\end{algorithm}

\subsection{Training Methodology}

\subsubsection{Environment Design}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{images/rl_environment_architecture.png}
  \caption[Overview of the Reinforcement Learning Environment]{Overview of the RL environment constructed for training the RL agent. The environment employs multi-threaded interactive and batch applications to replicate workloads generated by multiple serverless functions with shared access to persistent memory. These applications interact with the NVM Middleware through function calls using its programming interface. Meanwhile, in the background, the agent continuously iterates through a closed feedback loop, observing the environment's state and executing scaling actions based on its policy.}
  \label{fig:rl_environment_architecture}
\end{figure}

The environment architecture designed for training and evaluating the RL agent is depicted in Figure \ref{fig:rl_environment_architecture}. This architecture comprises several key components, including an interactive multi-threaded application, a batch multi-threaded application, the NVM Middleware, and Intel Optane PMem.

To simulate a multi-tenant serverless scenario, both applications are executed concurrently. Workload patterns for each application are derived from collected serverless traces. To emulate high concurrency levels typical in serverless environments, multiple threads within each application are employed to dispatch requests to the NVM Middleware via the API described in Section 3.3. Meanwhile, the NVM Middleware processes these requests in accordance with the workflow outlined in Section 3.2.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{images/rl_sequence_flow.png}
  % \caption[Reinforcement Learning Agent Process flow]{Illustration depicting the interaction between RL agent and its environment at each time step. At each time step, the agent's scaling action is performed before executing the next one-second window of I/O requests. Then the environment collects latency and throughput metrics to evaluate the action's efficiency and uses these metrics to return a reward signal that is used by the agent to improve its policy.}
  \caption[Reinforcement Learning Agent Process Flow]{Illustration depicting the interaction between the RL agent and its environment at each time step. During each time step, the agent executes a scaling action, followed by the processing of the subsequent window of I/O requests by the environment. Subsequently, the environment gathers latency and throughput metrics to assess the effectiveness of the action, providing a reward signal that the agent utilizes to refine its policy.}
  \label{fig:rl_sequence_flow}
\end{figure}

In order to model the time steps inherent in an RL process, the environment organizes the applications' requests into 1-second windows, processing one window per time step. Figure \ref{fig:rl_sequence_flow} illustrates the interactions between the RL agent and the environment at each time step. Beginning with a state observation from the preceding step, the agent communicates the intended action to the environment. Subsequently, the environment relays this action to the NVM Middleware, which then allocates resources accordingly. Upon successful execution of the action, the environment initiates processing for the next window of requests. Once all requests within the window are handled, the environment gathers metrics from the NVM Middleware and furnishes a new state observation along with a reward signal to the agent. The agent utilizes this reward to update its policy, perpetuating the iterative learning process.

\subsubsection{Function Approximation}

To address the challenge posed by the continuous state space in our environment, traditional Q-learning approaches become impractical due to the vast number of states that cannot be feasibly mapped into a Q-table. Consequently, we employ function approximation techniques to estimate the value of each action based on the state.

Specifically, we train nine separate regression models, each corresponding to one of the available actions, using stochastic gradient descent. This approach allows us to capture the underlying patterns in the data and generalize across states not encountered during training, enabling our agent to make informed decisions even in novel situations.

However, selecting appropriate hyperparameters for our regression models presents a significant challenge. Online training alone is insufficient for accurately assessing model performance, as it can be time-consuming and computationally intensive. To overcome this limitation, we adopt a batch learning approach with offline historical data.

By leveraging historical data collected from the environment, we can tune our models' hyperparameters and incorporate prior knowledge into our RL agent. This approach accelerates the learning process by bootstrapping our models with valuable insights gained from past experiences \cite{cano2017curator,vukosi2015improved}.

To construct our dataset, we deploy a non-optimal agent that performs random actions in the environment, capturing state-action-reward transitions. Following established machine learning practices, we split the dataset into training and testing sets and employ 5-fold cross-validation on the training set to evaluate model performance rigorously.

Additionally, we preprocess the features by standardizing them using the standard scaler and apply polynomial preprocessing to enhance the model's ability to capture non-linear relationships within the data.


\subsubsection{Proposed Q-Learning Algorithm}

Algorithm \ref{algo:q_learning_mw} outlines the Q-Learning process for training an agent to make optimal decisions in the environment. It takes the bootstrapped Q-value models $M_a$ for all actions $a$ and outputs the new learned models after training.

The algorithm initializes the training parameters and then iterates over a specified number of episodes. Within each episode, the environment is reset, and the agent interacts with it until the episode is complete. At each step, the agent observes the current state $s_t$, selects an action $a_t$ based on an $\epsilon$-greedy policy, takes the action, and observes the resulting reward $r$ and next state $s_{t+1}$.

The Q-value models are updated based on the observed reward and next state. If the episode is not done, the target Q-value is calculated using the reward and the maximum Q-value for the next state. If the episode is done, the target Q-value is simply set to the reward.

The model for the selected action $a_t$ is updated using the target Q-value, and the state is updated to the next state. Additionally, the exploration rate $\epsilon$ is decreased according to an exploration schedule.

\begin{algorithm}
  \small
  \caption{Q-Learning Algorithm}
  \label{algo:q_learning_mw}
  \SetAlgoLined
  \KwIn{Pre-trained Q-value models $M_a$ for all actions $a$}
  \KwOut{Learned Q-value models $M_a$ for all actions $a$}
  Initialize the training parameters $\alpha$, $\gamma$, $\epsilon$\;
  \For{$episode \leftarrow 1$ \KwTo $E$}{
    Reset the environment\;
    \Repeat{episode is done}{
      Observe the state $s_t$\;
      \tcp{Choose action $a_t$ using the $\epsilon$-greedy policy}
      Generate random number $r$ from uniform distribution in [0, 1]\;
      \If{$r < \epsilon$}{
        Select a random action $a_t$ from the action space \;
      }
      \Else{
        \For{each action $a$}{
            Predict Q-value $Q_a(s_t)$ using model $M_a$: $Q_a(s_t) \leftarrow M_a.predict(s_t)$ \;
        }
        Select action $a_t \leftarrow \argmax_a Q_a(s_t)$ \;
      }
      Take action $a_t$, observe reward $r$ and next state $s_{t+1}$\;
      \tcp{Update the Q-value model using reward and next state}
      \If{not done}{
        \For{each action $a$}{
          Predict Q-value $Q_a(s_{t+1})$ using model $M_a$: $Q_a(s_{t+1}) \leftarrow M_a.predict(s_{t+1})$ \;
        }
        Calculate target Q-value: $target \leftarrow r + \gamma \cdot \max_a Q_a(s_{t+1})$ \;
      }
      \Else{
        Set target Q-value to the reward: $target \leftarrow r$ \;
      }
      Update the model for action $a_t$ with the target Q-value: $M_{a_t}.partial\_fit(s_t, target)$ \;
      Update state: $s_t \leftarrow s_{t+1}$\;
    }
    Decrease $\epsilon$ according to exploration schedule\;
  }
\end{algorithm}

\section{Implementation}

The NVM Middleware, detailed in Section 3.3, is implemented using C++. We leverage PMEMKV from the Persistent Memory Development Kit \cite{scargall2020pmem} to facilitate reading and writing data into Intel Optane PMM. To manage concurrent operations efficiently, we utilize the non-locking, concurrent queue provided by the Intel Threading Building Blocks \cite{tbb:online} library for both the interactive and batch queues.

For the RL Environment, as described in Section 3.4.3, we adopt a hybrid approach employing C++ and Python. The environment itself is constructed in C++, aligning with the specifications outlined in Section 3.4.3. Conversely, the RL agent and the Q-Learning algorithm, also discussed in the same section, are developed using Python. We leverage the SGDRegressor model from the Scikit-learn\cite{scikitle61:online} library to facilitate the representation of our regression models for function approximation. Additionally, we employ Scikit-learn for hyperparameter tuning. To seamlessly integrate the C++ and Python components, we utilize pybind11\cite{pybind1111:online}.
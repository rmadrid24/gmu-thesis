\chapter{Related Work}

% In this thesis, we investigate the application of reinforcement learning for dynamically controlling the concurrency level on Intel Optane DC PMem, leveraging it as persistent storage for serverless workloads. 
This chapter provides a comprehensive review of the relevant literature within the scope of our study. The literature review is categorized into several domains, including research focusing on resource contention in Intel Optane DC PMem, studies addressing the constraints of current serverless storage services, and investigations utilizing reinforcement learning for optimizing resource allocation decisions.

\textbf{Optane DC PMem Resource Contention:} Prior research has highlighted resource contention issues within Intel Optane DC PMem \cite{yang2020empirical,wu2020ribbon}. Strategies to mitigate these challenges often involve limiting the number of concurrent threads accessing persistent memory. Yang et al. \cite{yang2020empirical} enhance the performance of an NVM-aware file system by limiting the number of writer threads per Optane PMM. Similarly, Li et al. \cite{wu2020ribbon} address the overhead incurred by multiple threads flushing data from CPU caches to persistent memory by implementing a runtime system that regulates thread concurrency.

Our work draws inspiration from the approaches mentioned above. We design the NVM Middleware as an integration layer that applies model-guided optimizations to leverage Intel Optane DC PMem effectively. Similar to Ribbon, our work controls concurrency levels on persistent memory and dynamically adjusts thread counts as necessary. However, the NVM Middleware introduces a novel approach beyond conventional concurrency controls. It implements a workload-aware concurrency mechanism that allocates distinct worker threads for different workload types, enabling performance segregation among workloads with diverse requirements. Furthermore, unlike Ribbon, the NVM Middleware incorporates latency and bandwidth service level agreements to proactively adjust worker thread allocations for each workload type, leveraging reinforcement learning to adapt to workload changes.

% To reduce the overhead of multiple threads performing CLF, they implement a runtime system that regulates the concurrency of threads performing CLF, and dynamically adjusts the thread count based on OPtane DC PMem bandwidth thresholds. This system decouples CLF from applications and applies optimizations for best performance.

% Our work borrows ideas from the works mentioned above. We design the NVM Middleware as an integration layer that applies model-guided optimizations for better usage of Intel OPtane DC PMem. Similar to Ribbon, our work controls the concunrrency level on persistent memro and dynamically adjusts thread as needed. However, the NVM MIddleware introduces a novel approach beyond conventional concurrency controls. It presents a workload-aware concurrency mechanism assigning distinct worker threads for different workload types. This enables the NVM Middleware to segregate performance among workloads with diverse requirements. Furthremore, unlike Ribbon, our work takes into account the different performance metrics and service level agreements of applications when deciding how to scale resources. Also, unlike RIbbon, our work uses reinforcement learning instead of threshold-based policies to better adapt to workload chages.

% In contrast, the NVM Middleware introduces a novel approach beyond conventional concurrency controls. 

% It presents a workload-aware concurrency mechanism assigning distinct worker threads for different workload types. This enables the NVM Middleware to segregate performance among workloads with diverse requirements. Furthermore, unlike Ribbon, the NVM Middleware incorporates latency and bandwidth service level agreements to proactively adjust worker thread allocations for each workload type.

\textbf{Limitations of Serverless Storage:} Research on serverless platforms has underscored inefficiencies stemming from limitations in existing cloud storage offerings. Several systems have emerged to alleviate communication bottlenecks in serverless functions.

Some systems minimize storage overhead by reducing traffic to remote storage and utilizing local communication mechanisms among related serverless functions \cite{akkus2018sand,romero2021faat}. SAND \cite{akkus2018sand} facilitates direct communication between related serverless functions running on the same server via a local message bus. FAA\$T \cite{romero2021faat} introduces a transparent and auto-scalable in-memory caching layer to prioritize data reuse and reduce calls to remote storage. Similarly, Cloudburst \cite{Sreekanti_2020} enhances data locality for serverless functions by implementing a key-value cache on each machine hosting function invocations.

Others focus on tailored storage solutions for data-intensive serverless workloads, bridging performance and capacity gaps between DRAM and persistent storage through multi-tier storage approaches \cite{klimovic2018pocket,wu2019autoscaling}. Pocket \cite{klimovic2018pocket} utilizes application hints to determine data tier storage, while Anna \cite{wu2019autoscaling} employs frequency analysis of key invocations to dynamically move data between tiers. These systems employ reactive policies to autoscale resources based on predefined thresholds.

In contrast, the NVM Middleware capitalizes on Intel Optane DC PMem's unique blend of persistent data and memory-like speeds to bridge performance and capacity gaps without necessitating data tiering policies. Moreover, it adopts a proactive approach to resource autoscaling, leveraging Reinforcement Learning to predict workload changes and preemptively adjust resources.

Additionally, studies like Tariq et al.'s work on Sequoia \cite{tariq2020sequoia} shed light on cloud storage service quality issues and propose frameworks for better scheduling of serverless function chains. While not storage-centric, this work underscores the importance of improving service quality in serverless environments. Similarly, Pisces \cite{180275} extends performance isolation policies by achieving per-tenant performance isolation and fairness in shared storage services, suggesting a potential research direction for enhancing the NVM Middleware.

\textbf{Reinforcement Learning-based Optimization Policies:} Cano et al. \cite{cano2017curator} introduced a novel application of Reinforcement Learning to enhance task scheduling policies in their cluster environments. Their approach involved employing linear regression models to approximate the Q-function, a fundamental component in RL algorithms. Additionally, they conducted offline pre-training of these linear models, guided by model selection and hyper-parameter tuning processes. By initializing the RL agent with pre-trained models, they facilitated a faster learning process by providing initial knowledge about the environment. Inspired by their methodology, we adopted similar strategies in implementing our RL agent and Q-learning process, albeit using polynomial regression models instead.

% \chapter[Related Work]{Related Work}

% \section*{Optane DC PMem}

% Previous works have identified the resource contention observed withing Intel Optane DC PMem. These works overcome this issue by limiting number of threads access persistent memory concurrently. the Yang et. al \cite{yang2020empirical} improve the performance of an NVM-aware file system by limiting the writer threads per Optane PMM. Similarly, Li et. al. identify the overhead introduced by having multiple threads flushing data from the CPU caches to persistent memory. The propose Ribbon, a runtime system that limits the number of threads performing CLF concurrently. Ribbon dynamically adapts the number of threads when the bandwidth reported by Optane DC PMem reaches certain minimum and maximum thresholds.

% The NVM middleware goes beyond the concurrency control mechanisms explained in the previous works. It proposes a workload-aware concurrency control mechanism that assings different worker threads for each workload type. This enables the NVm Middleware to isolate performance among workloads with different performance requirements. Additionally, unlike Ribbon, the NVM Middleware considers latency and bandwidth service level agreements to implement proactive policies that dynamically scale the the worker threads assigned to each workload type.

% \section*{Serverless Storage}

% Previous works have identified workloads that fail to run efficiently in serverless platforms due to the limitations of exising cloud storage offerings. These works have led to the design of several systems that minimizes the communication bottleneck in serverless functions.

% Some systems minimze the storage overhead by reducing the traffic sent to remote storage and using local communication mechanisms betwrrn related serverless functions running in the same server. SAND \cite{akkus2018sand} runs a local message bus to enable direct communication between related serverless functions running in the same server. FAA\$T \cite{romero2021faat} proposes a transparent and auto-scalable in-memory caching layer for serverless functions. This cache reduces the calls to remote storage by prioritazing data reuse. Similaryl, Cloudburst \cite{Sreekanti_2020} stateful serverless runtime provies better data locality for serverless functions by running a key-value cache on every machine that hosts function invocations.

% Other works have focused on building storage systems tailored for data-intensive serverless workloads. These systems overcome the performance and capacity gap between DRAM and persistent storage by combining multiple storage tiers. Pocket \cite{klimovic2018pocket} uses application hints to decide in which tier to store the applications data. Anna \cite{wu2019autoscaling} implements policies that analyze the frequency in which keys are invoked to move data between tiers. Furthermore, these systems implement reactive policies to autoscale resources when certain thresholds are met.

% In contrast, the NVM MIddleware leverages the unique combinaiton of data persistence with memory-like speeds offered by INtel OPtane DC PMem. This innovative technology aims to bridge the capacity and performance gap between DRAM and storage, eliminating the need to implement policies to move data among storage tiers. Furthermore, the NVM Middleware proposes a practive approach to autoscale resources, using Reinforcement Learning to predict workload changes and act in advance.

% Finally, other works have focused on analyzing the quality of services on cloud storage services. Tariq et. al. \cite{tariq2020sequoia} demonstrate how the poor scheduling techniques of major cloud providers affect the performance of serverless function chains. They implement Sequoia, a framework that enable cloud providers to define how serverless fucntions should be prioritized and scheduled to meet QoS objectives. While this work is not focused on storage systems, it leaves this as an open challenge. Pisces \cite{180275} goes beyond the performance isolation policies implemented in the NVM Middleware by achieving per-tenant performance isolation and fairness in shared storage services. This could be a future research avenue for our work.


% Previous works have proposed the development of new storage systems to overcome the limiations of existing cloud storage offerings \cite{klimovic2018pocket,wu2019autoscaling,10.14778/3587136.3587139}. These solutions overcome the performance difference between DRAM and storage by combining multuple storage medias, keeping hot data in DRAM and cold data in persistent storage. As a Pocket \cite{klimovic2018pocket} uses application hints to decide in which tier to store the application's data. Anna \cite{wu2019autoscaling} analyzes the frequency in which keys are invoked to move data between tiers. In contrast, this thesis proposes the use of the novel Optane DC PMem, which offers a unique combination of persistent storage with memory-like speeds. 
% The systems mentioned above also focus on provisioning resources dynamically to meet the wrokload demands. However, they implement reactive policies, taking when certain thresholds are met. In contrast, the NVM Middleware implements a proactive approach, using Reinforcement Learning to predict workload changes and act in advance. 

% \section*{Enabling Quality of Service}
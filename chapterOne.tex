
%% This file represents a sample first chapter of the main body of the dissertation
%%
%%**********************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either
%% expressed or implied; without even the implied warranty of
%% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall any contributor to this code be liable for any damages
%% or losses, including, but not limited to, incidental, consequential, or
%% any other damages, resulting from the use or misuse of any information
%% contained here.
%%**********************************************************************
%%
%% $Id: chapterOne.tex,v 1.6 2006/08/24 21:13:45 Owner Exp $
%%

% A first, optional argument in [ ] is the title as displayed in the table of contents
% The second argument is the title as displayed here.  Use \\ as appropriate in
%   this title to get desired line breaks
\chapter[Introduction]{Introduction}

\section{Motivation}

Serverless computing has experienced significant growth in recent years, emerging as a prominent paradigm for cloud application development. In serverless architecture, developers package their code into stateless functions and deploy it on the serverless platform without the responsibility of managing underlying infrastructure. The serverless platform dynamically scales resources to execute these tasks, billing users only for the resources consumed during function invocations. These characteristics make serverless computing attractive for a diverse range of applications, including web and IoT microservices \cite{gan2019opensource}, video processing \cite{fouladi2017encoding}, data analytics \cite{gimenez2019framework,carver2020wukong,klimovic2018pocket}, machine learning \cite{carreira2019cirrus,feng2018exploring}, and storage applications \cite{10.14778/3587136.3587139,jonas2019cloud}.

Statelessness is a fundamental aspect of serverless functions, as they do not retain state between invocations \cite{jonas2019cloud}. This intentional design choice contributes to high elasticity by eliminating the need to store state within function instances. It enables cloud providers to execute functions in parallel, thereby optimizing resource utilization. Any required data between function invocations must be stored remotely, adhering to the stateless nature of serverless architecture.

While the stateless nature of serverless computing is crucial for achieving high elasticity, it imposes limitations on the types of applications that can efficiently operate on serverless platforms. Previous studies \cite{jonas2019cloud,pu2019shuffling,gan2019opensource} have identified that data-intensive applications, including data analytics, machine learning workflows, and databases, face challenges due to capacity and performance gaps in existing storage services. For instance, object storage services like AWS S3 offer cost-effective long-term storage but suffer from high access latencies. Conversely, in-memory clusters like AWS ElastiCache boast low access latencies and high throughput but are costly and lack transparent provisioning. Meanwhile, key-value databases such as AWS DynamoDB deliver high throughput but are expensive and slow to scale.

Given the constraints of current storage solutions, prior research emphasizes the necessity for a serverless storage service capable of accommodating a diverse array of workloads on serverless platforms. These studies underscore three essential requirements for such a service. Firstly, it must offer low latency and high throughput for various object sizes and data access patterns. Secondly, it should feature transparent provisioning and scalability to accommodate workload fluctuations. Finally, it is imperative that the serverless storage service guarantees isolation and predictable performance across applications and tenants to align with the requirements outlined in service level agreements (SLAs) \cite{jonas2019cloud,klimovic2018pocket}.

To address the requirements outlined in prior research for a serverless storage service, this study focuses on leveraging Intel Optane DC Persistent Memory (PMem) technology. Optane PMem represents a significant advancement in non-volatile memory, offering a blend of persistence and high performance \cite{IntelOp15:online}. Installed on the memory bus alongside traditional DRAM, Optane PMem provides a byte-addressable interface and achieves speeds comparable to DRAM, albeit slightly slower. However, its distinguishing feature lies in its higher capacities and ability to retain data during system shutdowns or power loss, effectively functioning as persistent storage with memory-like speeds \cite{yang2020empirical,izraelevitz2019basic}.

This unique combination of characteristics positions Optane PMem as an ideal solution for accelerating data-intensive workloads in serverless platforms. Therefore, this thesis analyzes the efficient utilization of Optane PMem for developing a serverless storage service that meets the requirements of low latency, high throughput, transparent provisioning, scalability, and performance predictability across applications and tenants, as outlined in service level agreements (SLAs).

% Intel Optane DC PMM differs from DRAM in two ways. First, there is a mismatch between the CPU cacheline access granularity (64-byte) and the 3D-XPoint media access granularity (256-byte) in Intel Optane DC PMM. This difference can lead to write or read amplification if the data access is smaller than 256 bytes. Second, to balance the gap in access granularity, the Intel Optane DC PMM implements a small (16KB) write-combining buffer to merge small writes and reduce write amplification. However, the buffer’s limited capacity (16 KB) can cause contention within the device, limiting its ability to handle access from multiple threads simultaneously.

\section{Research Questions}

With the introduction of Intel Optane PMem, researchers have delved into understanding its characteristics, capabilities, and limitations \cite{izraelevitz2019basic, yang2020empirical, wu2020ribbon}. Initial expectations presumed that Intel Optane DC PMM would behave similarly to DRAM but with lower performance (higher latency and lower bandwidth). However, recent studies suggest that it should not be regarded simply as a “slower, persistent DRAM”. Unlike DRAM, Optane DC PMM displays complex behaviors, and its performance varies based on factors such as access size, access type (read vs. write), and concurrency levels.

One significant difference between Optane PMem and DRAM is that Optane PMem's performance does not scale with higher thread count. This disparity stems from the mismatch between the CPU cacheline access granularity (64-byte) and the 3D-XPoint media access granularity (256-byte) in Optane PMem DIMMs. To address this discrepancy, Intel PMem DIMMs implement a small write-combining buffer to merge small writes and reduce write amplification. However, the buffer’s limited capacity (16 KB) can lead to contention within the device, restricting its ability to handle access from multiple threads simultaneously.

The complex behavior of Intel Optane DC PMM introduces intriguing challenges for constructing a serverless storage service using this technology. Firstly, serverless platforms typically share resources among tenants to execute their applications. These applications vary considerably in multiple aspects, including data access and processing methods, and their performance demands. Secondly, these workloads can experience significant spikes in demand and undergo dramatic changes over time. Understanding how these large-scale variations impact Optane PMem performance can aid in developing an efficient serverless storage service based on this technology.

Given the wide heterogeneity of applications running on serverless platforms, we focus our work on two main types of applications: interactive and batch applications. Interactive applications, such as web-based platforms, facilitate real-time interactions between users and applications. Low latency is crucial to ensure prompt processing of user input and delivery of real-time feedback. Conversely, batch applications, such as data analytics jobs, prioritize high throughput to efficiently process large volumes of data.

Consequently, this thesis addresses the following research questions:

\begin{itemize}
    \item What is the performance degradation (in terms of latency and throughput) observed with Optane PMem when interactive and batch applications concurrently share the storage media? How does this performance degradation impact the service level agreements of serverless applications?
    \item How can we address the limitations of Optane PMem to maximize its efficiency when used as storage media for serverless workloads?
    \item How can we optimize and fine-tune Intel Optane PMem to ensure consistent adherence to service level agreements as workload dynamics evolve over time?
\end{itemize}

\section{Contributions}

Our experiments offer valuable insights into the behavior of Optane PMem when employed as persistent storage for serverless workloads. Firstly, we observe that sharing Optane PMem among multi-threaded interactive and batch applications sim results in performance degradation, characterized by increased latency and reduced bandwidth. This outcome aligns with expectations, considering the contention issues experienced by Optane PMM with higher thread counts. Secondly, we find that the performance degradation in Optane PMM varies depending on the workload, affecting either latency or bandwidth more prominently. This discrepancy implies that certain applications may experience greater QoS impacts than others.

To address the limitations of Intel Optane PMM, we introduce a control layer designed to optimize its utilization within serverless environments. Named the NVM Middleware, this layer integrates with storage services to manage Optane PMM access effectively, mitigating contention. The NVM Middleware implements distinct concurrency levels for interactive and batch applications, ensuring performance isolation across different application types. Leveraging machine learning techniques, the NVM Middleware dynamically adjusts these concurrency levels to meet current demand and adapt to changing workloads. We advocate for the use of online reinforcement learning algorithms, given the evolving data access patterns typical of serverless workloads.

Our contributions can be summarized as follows:

\begin{itemize}
    \item We conduct an experimental study that assesses the capabilities and limitations of Intel Optane PMem as persistent storage for serverless workloads, a scenario not previously explored.
    \item We introduce the NVM Middleware, a control layer designed to reduce contention within Optane PMem while maintaining performance isolation among diverse application types.
    \item We propose a Reinforcement Learning model to train an agent capable of dynamically scaling and provisioning resources within the NVM Middleware to meet service level agreements amidst changing workloads.
    \item Finally, we present empirical evidence demonstrating the effectiveness of our proposed solution.
\end{itemize}

\section{Structure}

The structure of this research paper is outlined as follows:

Chapter 2 provides background information on Intel Optane DC PMem, serverless computing, and Reinforcement Learning (RL). It elaborates on the techniques employed for implementing the RL model within the NVM Middleware.

Chapter 3 details the work carried out on the NVM Middleware. Beginning with a discussion on the essential characteristics of proper serverless storage, the chapter describes how the NVM Middleware contributes to achieving these characteristics while ensuring efficient utilization of Intel Optane DC PMem. It includes an overview of the architecture and programming interface of the NVM Middleware, followed by insights into the Reinforcement Learning model and the Q-Learning algorithm designed to train the NVM Middleware to dynamically adjust concurrency levels under shifting workloads.

Chapter 4 presents an evaluation of the NVM Middleware, encompassing an assessment of its concurrency control mechanism and the performance of the Q-Learning algorithm.

Chapter 5 explores related work in the fields of Intel Optane DC Persistent Memory, serverless storage, and reinforcement learning for dynamic resource allocation. This chapter also discusses the relationship between our work and previous studies, highlighting similarities and differences.

Chapter 6 discusses limitations inherent in this study and proposes potential avenues for future research and development.

Finally, Chapter 7 concludes the research paper by summarizing the presented work and its implications.
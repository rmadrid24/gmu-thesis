\chapter[Evaluation]{Evaluation}

In this chapter, the NVM Middleware and the Q-Learning Model. We utilize the environment delineated in Section 3.4.3 to run a set of experiments.

\section{Experimental Setup}

\subsection{Platform}

\begin{table}[ht]
    \centering
    \label{table:platform_specifications}
    \caption{Experimental Platform Specifications}
    \begin{tabular}{|l|l|}
      \hline
      Processor & Intel\,\textsuperscript{\tiny\textregistered} Xeon\,\textsuperscript{\tiny\textregistered} Gold 6252   \\\hline
      Sockets & 2 \\\hline
      Cores per socket & 24  \\\hline
      Threads per core & 2 \\\hline
      Numa nodes & 2 \\\hline
      CPU Frequency & 2.7 GHz (3.7 GHz Turbo frequency) \\\hline
      L1d cache & 1.5 MiB  \\\hline
      L1i cache & 1.5 MiB  \\\hline
      L2 Cache & 48 MiB  \\\hline
      L3 Cache & 71.5 MiB  \\\hline
      DRAM & 16 GB DDR4 DIMM x 6 per socket  \\\hline
      Persistent Memory & 128 GB Optane PMM x 6 per socket  \\\hline
      Operating System & Ubuntu 20.04.4 LTS (Focal Fossa)  \\
      \hline
    \end{tabular}
\end{table}

The experimental platform utilized in this study is detailed in Table \ref{table:platform_specifications}. It features an Intel,\textsuperscript{\tiny\textregistered} Xeon,\textsuperscript{\tiny\textregistered} Gold 6252 processor with 2 sockets, each hosting 24 cores and 2 threads per core, totaling 2 NUMA nodes. Each socket is equipped with three memory channels, housing 16 GB DDR4 DIMMs and 128 GB Optane PMMs. In aggregate, the system comprises 192 GB of DRAM and 1.5 TB of Optane persistent memory. To mitigate the NUMA effect, one socket is designated for running the NVM Middleware threads, while the other handles the interactive and batch applications, as described in Section 3.4.3.

\subsection{Optane DC PMem Configuration}
As outlined earlier, this thesis concentrates on exploring the persistent capabilities of Optane DC PMem. Consequently, Optane DC PMem is employed in the App Direct Mode throughout our experiments. To facilitate the utilization of persistent memory, we expose it via an xfs filesystem configured in dax mode, thereby bypassing the page cache. Additionally, we enhance memory management and performance by configuring the persistent memory with huge pages (2MiB) \cite{Speeding28:online}. Lastly, we deploy a PMEMKV database with a capacity of 600GB, configured with its persistent concurrent engine.

\subsection{Workload Generators}

We deploy the interactive and batch applications outlined in Section 3.4.3 utilizing two distinct workload generators.

\subsubsection{Yahoo! Cloud Serving Benchmark (YCSB)}

YCSB \cite{GitHubba9:online} is a multi-threaded benchmark specifically tailored for assessing cloud-based databases and storage systems. Utilizing YCSB, we emulate interactive applications by generating small byte requests. To vary the workload characteristics, we modify parameters such as the read-to-write ratio, request distribution, and client threads. Leveraging the C++ version of YCSB, we extend its functionality to facilitate API calls to the NVM Middleware.

\subsubsection{Serverless Trace Replay}

We develop the Serverless Trace Replay tool in-house to replicate workloads typically encountered in real-world serverless environments. This tool operates by reading a file containing workload traces and executing them accordingly. To simulate multiple serverless functions, the tool spawns multiple threads to handle incoming requests.

For emulating an interactive application, we utilize traces collected from Azure Functions, sourced from a dataset available in \cite{GitHubAz35:online}. This dataset (described in more detail in \cite{romero2021faat}) offers a comprehensive log of Azure Function blob accesses over HTTPS recorded between November and December 2020. Specifically, our experiments utilize requests recorded on December 6, 2020, with a specific emphasis on those involving small data access sizes (less than 1 KB), which typically signify interactive application behavior.

For modeling a batch application, we collect traces from Wukong, a serverless parallel computing framework \cite{carver2020wukong}. The traces are acquired by executing a Single Value Decomposition job for a 128kx128k matrix on Wukong and capturing the resulting I/O requests generated by the framework. This dataset offers a precise representation of throughput-oriented serverless data-analytics applications, characterized by significant parallelism and substantial data access sizes spanning from 4KB to 200MB.

% To further amplify the concurrency of requests directed to the NVM Middleware, we accelerated the pace of the traces by a factor of 5 compared to their original timing.

\section{Efficiency of the Workload-Aware Concurrency Control Mechanism}

We evaluate the effectiveness of the workload-aware concurrency control mechanism embedded within the NVM Middleware compared to a baseline scenario devoid of concurrency control. In the baseline setting, concurrency control is inactive, permitting a maximum of 200 concurrent data accesses on Optane DC PMem. This assessment involves deactivating the reinforcement learning agent and system monitoring, with a primary focus on the 99th percentile latency and throughput observed by client applications.

Six experiments are conducted, running YCSB and SVD Trace Replay concurrently. Each YCSB experiment varies parameters such as data access size (64B, 128B), read-to-write ratio (50-50, 100-0), and data request distribution (zipfian, uniform), while SVD Trace Replay maintains consistent settings across experiments. Both applications are executed with 100 client threads.

In each experiment, the baseline scenario is executed without concurrency control, followed by 42 additional tests exploring various combinations of interactive and batch threads within the NVM Middleware. Each run maintains a fixed combination of interactive ($I$) and batch ($B$) threads. Subsequently, the 99th percentile latency observed by YCSB requests and the overall throughput reported by SVD Trace Replay are recorded. Results are presented in Appendix A.

Our observations indicate substantial benefits from the concurrency control implemented by the NVM Middleware across most scenarios. Relative to the baseline, the NVM Middleware demonstrates potential enhancements of up to 98\% in 99th percentile latency and 86\% in throughput. Notably, certain thread combinations achieve sub-millisecond access latencies with a more predictable behavior, significantly improving application performance. However, improper thread configuration within the NVM Middleware, either insufficient or excessive, results in performance degradation exceeding that of the baseline. This emphasizes the criticality of meticulously selecting the optimal thread combination, as an incorrect choice may yield similar or inferior results compared to operating without concurrency control.

A key query arising from these findings concerns determining the optimal thread combination. We observe that prioritizing interactive threads yields sub-millisecond access latencies but compromises peak throughput. Conversely, increasing batch threads to enhance throughput metrics leads to higher access latencies. Addressing this dilemma entails selecting a combination of interactive and batch threads that satisfies both latency and throughput SLA metrics, a topic elaborated upon in the subsequent section.

% First, all the baseline runs highlight the performance degradation observed when no concurrency control is performed over Optane DC PMem. This is due to the resource contention within Optane DC PMem caused by the high number of concurrent threads accessing the persistent memory. In all the experiments, the throughput stays low, while the 99th access latency spikes by orders of magnitude. The latency seems more affected than the throughput. This could be because the SVD 128x128 uses a bigger block sizes which increases the contention within the OPtane PMem device.

% Our observations reveal that in most scenarios, both workloads derive substantial benefits from the concurrency control implemented by the NVM Middleware. Relative to the baseline, the NVM Middleware demonstrates the potential to enhance the 99th latency and throughput by up to 98\% and 86\%, respectively. Notably, the figure illustrates that the performance of applications is generally improved across most thread combinations. However, improper configuration of threads within the NVM Middleware, either too few or too many, leads to performance degradation surpassing that of the baseline. This underscores the importance for operators to meticulously select the optimal combination of threads, as an incorrect choice can yield similar or inferior results compared to operating without any concurrency control.

% We observe that the latency reported by 

% while the resource contention is still present with the NVM Middleware, 
% the baseline shows that the 99th latency can vary by X\% orders of magnitude, while the NVM Middleware exhibits a more controlled and predictable access latencies. Notably, the figure illustrates that the performance of applications is generally improved across most thread combinations. However, improper configuration of threads within the NVM Middleware, either too few or too many, leads to performance degradation surpassing that of the baseline. This underscores the importance for operators to meticulously select the optimal combination of threads, as an incorrect choice can yield similar or inferior results compared to operating without any concurrency control.

% A significant query stemming from these findings pertains to how an operator can determine the optimal thread combination. We observe that giving more priority to the interactive threads yield sub-millisecond access latencies but fails to achieve peak throughput. Conversely, as we start to increase the thrhougput metrics by addin more batch threads, the access latencies starts to increase. To address this dilemma, the ideal approach involves selecting the combination of interactive and batch threads that satisfies both latency and throughput SLA metrics, a topic further elaborated upon in the subsequent section.


% We observe that combining 16 interactive threads with fewer than 8 batch threads yields superior latency but fails to achieve peak throughput performance. Conversely, any combination with more than 16 batchs threads achieves peak throughput but incurs elevated access latencies. To address this dilemma, the ideal approach involves selecting the combination of interactive and batch threads that satisfies both latency and throughput SLA metrics, a topic further elaborated upon in the subsequent section.

\section{Meeting SLA performance using RL}
% We now provide an evaluation of the RL-driven policies to balance the number of interactive and batch threads to meet latency and throughput SLAs. In this experiment, the goal of the NVM MIddleware is to meet pre-defined SLA objectives under changing workloads. To do this, we build 4 different workloads, each consisting of an latency-sensitive and throughput-oriented application running concurrently in the environment. The workloads are described in Appendix \ref{appendix:c}. Using the Q-Learning algorithm described in \ref{algo:q_learning_mw}, we train the RL agent to learn the optimal combination of interactive and batch threads that maximimzes performance and meets pre-defined latency and throughput SLA objectives for each workload. Finally, we measure the agent's ability to predict and adapt to workload changes in an unknown environment where the workloads are randomly alternated. 

% We start the learning process by performing model selection and hyperparameter tuninng on the 9 linear regression models used by the RL agent. Table \ref{table:hyperparameter_tuning} shows the options used for this process. To do this, we generate a dataset of transitions in the environment by running a non-optimal random agent on the environment. We run 150 episodes of each workload and let the random agent take random actions on the environment, logging the transitions and the rewards obtained by each transition. Since each linear regression model is supposed to approximate $Q(s, a)$, we build 9 different sub-datasets, where each dataset contains only the transitions correspnonding to a specific action taken. We use each sub-dataset to select the right model and tune the hyperparameters of a model $M_a$, choosing from the parameters that best fits the data. The resulting linear regression models are described in Table \ref{table:per_model_parameters}.

% Using the tuned linear regression models, we proceed to run the Q-learning algorithm for each workload using the parameters outlined in Table \ref{table:rl_training_parameters}. We address the exploration-exploitation dilemma by starting with epsilon 1 and decaying it after each episode. This causes the agent to fully explore the state space at the beginning of the training and exploit this knowledge towards the end. We observe that different phases require different training episodes to converge to an optimal pattern. We believe this is expected given that the dataset use to pre-train the models was generated with a non-optimal policy. The random agent might have been stuck in a non-optimal loop of actions and might have not generated good training samples. Therefore, the agent requires additional training to fully capture the characteristics of these phases. For earch phase, we analyze the last three episodes to determine the combination of threads to which the agent converges. We choose the last three episodes because at that point the agent is exploiting the knowledge obtained from previous episodes.

% We evaluate the performance of the traine RL agent against two baseline scenarios, one where we disable the NVM MIddleware concurrency control and another where we fix the NVM MIddleware to use 15 interactive and batch threads. In this experiment, we perform three tests running a long-rrun simulation consisting of 4,000 steps, changing the workload every 200 steps. Finally, for each test, we capture the min,25,50,75, and max throughtput, 99th latency (observed by NVM Middlewre), and rewards observed (observed by the environment). The results presented in Appendix \ref{appendix:e}

% Our observations demonstrates how the RL-driven policies add extra performance benefits under changing workloads. While adding the NVM MIddleware with a fixed policy already exhibits performance improvements, the dynamic control implemented by the RL agent improves the performance even furthre. The increased rewards (Table \ref{table:eval_results_reward}) observed by the RL agent indicate that it does a better job at maximizing performance and meeting target SLAs. This behavior is reflected in the throughtput achieved by the RL agent, where the NVM MIddleware achieves the throughtput target for 50\% of the steps . In the other hand, keeping the NVM MIddleware with a fixed policy only achieves the throughput target for less than 25\% of the steps. Furthermore, even thought the NVM MIddleware does not achieve the target throughput in all the steps, the overall throughput is closer to the target compared to the other two scenarios.

% Finally, in this experiment, the 99th percentile exhibited by both scenarios using the NVM MIddleware did not work as expected. We believe the problem was caused because the interactive workloads chosen for this experiment did not stresss the system enough, which is why the scenario with no control performs better. We discuss more about this topic in the next section.

We present an evaluation of RL-driven policies aimed at balancing the number of interactive and batch threads to meet latency and throughput SLAs within the NVM Middleware. This experiment assesses the NVM Middleware's ability to achieve predefined SLA objectives amidst varying workloads. To this end, we construct four distinct phases, each comprising a latency-sensitive and throughput-oriented application executed concurrently in the environment (see Appendix \ref{appendix:c} for phase details). Utilizing the Q-Learning algorithm (outlined in Algorithm \ref{algo:q_learning_mw}), we train the RL agent to determine the optimal combination of interactive and batch threads that maximizes performance while meeting predefined latency and throughput SLA objectives for each phase. We then evaluate the agent's ability to predict and adapt to workload changes in an unknown environment where phases are randomly alternated.

\subsection*{Convergence of the RL Agent}

We commence the learning process (see Appendix \ref{appendix:d} for details) by conducting model selection and hyperparameter tuning on the nine linear regression models employed by the RL agent (refer to Table \ref{table:hyperparameter_tuning} for tuning options). This entails generating a dataset of transitions in the environment by executing a non-optimal random agent on the environment for 150 episodes of each workload. Subsequently, we utilize these transitions to select the appropriate model and tune hyperparameters for each action. The resulting linear regression models are summarized in Table \ref{table:per_model_parameters}.

With the tuned linear regression models, we proceed to execute the Q-learning algorithm for each workload using the parameters detailed in Table \ref{table:rl_training_parameters}. To address the exploration-exploitation dilemma, we initialize the epsilon value to 1 and decay it after each episode. This strategy facilitates comprehensive exploration of the state space early in training, followed by exploitation of acquired knowledge. We note that different phases require varying numbers of training episodes to converge to an optimal pattern, likely due to the initial dataset's generation with a non-optimal policy. Initially, we conduct empirical tests by fixing the NVM Middleware threads to manually ascertain the optimal combination. Subsequently, we execute Q-learning and analyze the last three episodes of each phase to determine the convergent combination of threads.

Our observations indicate that the RL agent is capable of learning the optimal combination of interactive and batch threads for each workload. The learned combination of threads matches our empirical results across all workloads. For Workload A, the agent converges to utilizing approximately 10 interactive and batch threads each. Similarly, for Workload B, the agent converges to approximately 10 interactive and batch threads. For Workload C, the agent converges to approximately 7 interactive and 3 batch threads, while for Workload D, it converges to approximately 15 interactive threads and 5 batch threads.

\subsection*{RL Agent Evaluation}

We evaluate the trained RL agent's performance against two baseline scenarios: one with disabled concurrency control in the NVM Middleware and another with a fixed policy of 15 interactive and batch threads. Each test comprises a long-run simulation spanning 4,000 steps, with the workload changing every 200 steps. We capture the minimum, 25th, 50th, 75th, and maximum throughput and tail latency observed by the NVM Middleware, as well as the environment rewards for each test (results presented in Appendix \ref{appendix:e}). Figure \ref{fig:long_run_eval} illustrates the dynamic behavior of the RL agent as it adapts the combination of interactive and batch threads in response to the current workload patterns learned during training.

Our observations demonstrate the added performance benefits of RL-driven policies under varying workloads. While incorporating the NVM Middleware with a fixed policy yields performance improvements, dynamic control implemented by the RL agent further enhances performance. Increased rewards (Table \ref{table:eval_results_reward}) observed by the RL agent indicate superior maximization of performance and adherence to target SLAs. This is reflected in the achieved throughput (Table \ref{table:eval_results_tp}), with the NVM Middleware meeting the throughput target for 50\% of steps, compared to less than 25\% for the fixed policy scenario. Furthermore, although not achieving the target throughput in all steps, the NVM Middleware's overall throughput is closer to the target compared to the other scenarios.

However, in this experiment, the 99th percentile latency (Table \ref{table:eval_results_latency}) exhibited by both scenarios using the NVM Middleware did not meet expectations. We attribute this issue to the chosen interactive workloads not sufficiently stressing the system, leading to better performance in the scenario with no concurrency control. Further discussion on this topic is provided in the subsequent section.

% indicate that, while keeping a fixed configuration of the NVM Middleware is an improvement over not having concurrency control, the RL agent optimizes the NVM Middleware even furthre under changing workloads. 


% This confitms the theoroy that keeping a fixed policy within the NVM MIddleware is not ideal to match the dynamic nature of serverless workloads. The reward benefits are also reflected in the throughtput, where 

% While keeping a fixed configuration of the NVM MIddleware is already an improvement over not having concurrency control over persistent memory, the RL agent improves the NVM Middleware's performance even more. The lower rewards (Table \ref{table:eval_results_reward}) observed by the RL scenario imply that the RL-driven policies does a better job at meeting SLAs compared to the other two scenarios. This confirms the theory that keeping a fixed policy is not feasible for the dynamic behavior of serverless workloads. The 


% % We now evaluate our trained model in two scenarios, one where we alternate workloads. We compare the agent's performance against a baseline where we fix the combination of threads in the NVM MIddleware to 15 interactive and 15 batch threads.

% % Phase 1
% % We observe that the RL agent converges to 10 interactive threads and 10 batch threads. The results are showing in figure X.

% % Phase 2
% % We observe that the RL agent converges to 10 interactive threads and 10 batch threads. The results are showing in figure X.

% % Phase 3
% % We observe that the RL agent converges to 10 interactive threads and 10 batch threads. The results are showing in figure X.

% % Phase 4
% % We observe that the RL agent converges to 10 interactive threads and 10 batch threads. The results are showing in figure X.



% Once the RL agent is trained, we evaluate its performance against a baseline scenario where no learning is done. The experiment consists of a long-running simulation consisting of 400 steps, changing the workloads every 200 steps. In the baseline scenario, we fix the NVM Middleware interactive and batch threads to 15 threads for the whole experiment. In the RL scenario, the agent uses the knowledge obtained from training and adapts the interactive and batch threads as workload changes. We perform two RL experiments, one where we alternate the workloads sequentially and another changing the workloads randomly. For the three tests, we collect the reward on each step using the calculations described in \ref{algo:reward_calculation}.

% We evaluate the performance of the traine RL agent against two baseline scenarios, one where we disable the NVM MIddleware concurrency control and another where we fix the NVM MIddleware to use 15 interactive and batch threads. In this experiment, we perform three tests running a long-rrun simulation consisting of 4,000 steps, changing the workload every 200 steps. Finally, for each test, we capture the min,25,50,75, and max throughtput, 99th latency, and  rewards observed by the NVM MIddleware

% We evaluate its performance in a long-running simulation consisting of 400 steps, changing the workloads every 200 steps. 

% Furthermore, we compare the performance of the RL agent agains two baseline scenarios, one with no concurrency control and another fixing the NVM Middleware to 15 interactive threads

% For both, the baseline and the RL experiments, we capture the rewards using the calculatino defined in Algorithm \ref{algo:reward_calculation}.

% Table X shows the different percentiles (5,25,50,75,99) of the rewards for the 4000 steps in each experiment. THe lower rewards observed by the baseline indicated that the missed the SLA objectives more times than the RL agent. This means that keeping a fixed policy is not optiomal in the long run. The RL agent does a better job in meeting SLAs regardless of the order in which the workloads are changed. This means that the agent fully grasped the characteristics of each workloads and adjusts the resources once certain patterns are observed. Furthermores, Figure X shows the performance of each experiment.
